include::license.txt[]

:language: C

비동기 I/O에 대한 간단한 설명
----------------------------------------

대부분의 초보 프로그래머들은 Blocking I/O 호출로 프로그래밍을 시작합니다.

호출 하였을 때 작업이 완료될 때 까지 반환 되지 않거나, 네트워크 스택이 응답을 포기할 만한 시간이 흐를 때 까지
반환 되지 않을 경우, 이 I/O 호출을 _동기적(synchronous)_이라고 합니다.

예를 들어, TCP 연결을 위해 "connect()"를 호출할 때
여러분의 운영체제는 TCP 연결의 상대방 호스트에 SYN 패킷 하나를 쌓습니다(queue). 
이 호출은 여러분의 운영체제가 상대 호스트로부터 SYN ACK 패킷 하나를 받거나
충분한 시간이 흘러 받기를 포기할 때까지 어플리케이션으로 제어를 돌려주지 않습니다.

다음은 blocking 네트워크 호출을 이용한 매우 간단한 클라이언트 예제입니다.
예제는 www.google.com으로의 연결을 열어 간단한 HTTP request를 보낸 뒤,
response를 표준 출력(stdout)으로 보내는 동작을 합니다.

//BUILD: SKIP
.Example: A simple blocking HTTP client
[code,C]
-------
include::examples_01/01_sync_webclient.c[]
-------

위 코드에서 사용된 네트워크 호출은 모두 _Blocking_ I/O를 수행합니다.
gethostbyname은 www.google.com을 IP로 분석하는데 성공하거나 실패할 때까지 반환되지 않습니다.
connect는 호스트에 연결될 때까지 반환되지 않습니다.
recv는 상대방 호스트로부터 데이터를 받거나 연결이 닫힐 때까지 반환되지 않습니다.
send는 적어도 커널의 쓰기 버퍼(write buffers)에 출력을 flush하기 전까지는 반환되지 않습니다.

자, Blocking I/O가 꼭 나쁜 것 만은 아닙니다.
여러분의 프로그램이 동시에 수행할 다른 작업이 딱히 없다면
Blocking I/O는 맡은 바 소임을 잘 해낼 것입니다.
하지만 한번에 여러 연결을 처리해야 하는 프로그램을 만들어야 한다고 생각해봅시다.
좀 더 명확한 예제를 위해 첨언하자면,
두 개의 연결로 부터 입력을 읽고자 하는데 어떤 연결이 먼저 입력을 받을지 모른다고 합시다.
아래와 같이 할 수는 없습니다.

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Bad Example
[code,C]
-------
/* This won't work. */
char buf[1024];
int i, n;
while (i_still_want_to_read()) {
    for (i=0; i<n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n==0)
            handle_close(fd[i]);
        else if (n<0)
            handle_error(fd[i], errno);
        else
            handle_input(fd[i], buf, n);
    }
}
-------

그 이유는, 만약 데이터가 fd[2]에 먼저 도착한다면 여러분의 프로그램은
fd[1]로부터 데이터를 받아 모두 읽을 때까지 fd[2]를 읽기 시작하는 것 조차 못할 것이기 때문입니다.

때로는 멀티쓰레딩(multi-threading)이나 멀티프로세스(multi-process) 서버로
이 문제를 해결하기도 합니다.
멀티 스레딩을 수행하기 위한 가장 쉬운 방법 중 하나는
개별 프로세스(혹은 쓰레드)가 각 연결을 담당하는 것입니다.
각 연결을 담당하는 프로세스가 있기 때문에, 한 연결을 기다리는 Blocking I/O 호출이
다른 연결의 프로세스를 block 시키지 않을 것입니다.

다음은 또 다른 예제 프로그램입니다. 이 프로그램은 일반적인 서버로서 47013 포트를 열어
TCP 연결을 기다리는데(listen), 한번에 한 줄의 입력을 읽어(read) ROT13 암호화를 한 뒤
상대방 호스트에 씁니다(write). 이 프로그램은 들어오는 각 연결을 위해 새로운 프로세스를 생성하는데,
Unix의 fork() 호출을 사용합니다.

//BUILD: SKIP
.Example: Forking ROT13 server
[code,C]
-------
include::examples_01/01_rot13_server_forking.c[]
-------

그렇다면, 한번에 여러 연결을 처리하기 위한 완벽한 해결책이 나온건가요?
그럼 이제 이 책은 그만 쓰고 다른 걸 하러 가도 괜찮을까요?

꼭 그렇진 않습니다.

우선, 플랫폼에 따라 프로세스 생성(그리고 쓰레드 조차)에 많은 비용이 들 수 있습니다.
실제 환경에서는 새로운 프로세스를 생성하는 것 보다는 스레드 풀(thread pool)을 사용하려 할 테지만,
좀 더 기본적인 내용을 보자면, 쓰레드는 여러분이 원하는 만큼 확장(scale)되지 못합니다.
만약 여러분의 프로그램이 수천 개 혹은 수만 개의 연결을 동시에 처리해야 한다면,
수만 개의 쓰레드를 처리하는 일이 CPU당 몇 개의 쓰레드가 있을 때만큼 효율적이지는 않을 것입니다.

하지만 쓰레드를 이용한 방식이 정답이 아니라면, 여러 연결을 처리하기 위해 무엇을 사용해야 할까요?
유닉스의 전통을 따르자면, 여러분의 소켓을 _nonblocking_으로 만들 수 있습니다.
이를 위한 유닉스 호출은 다음과 같습니다.
[code,C]
------
fcntl(fd, F_SETFL, O_NONBLOCK);
------
fd는 소켓을 위한 파일 기술자(file descriptor)입니다.  footnote:[파일 기술자(file descriptor)란
여러분이 소켓을 열었을 때 그 소켓에 커널이 할당한 번호를 뜻합니다. 이 번호는 Unix 호출에 해당 소켓을 참조시킬 때 사용됩니다.]
일단 fd(소켓)를 nonblocking으로 만들었다면, 그 때부터 fd로 네트워크 호출을 할 때마다 해당 작업을 바로 완료시키거나
"지금은 더 이상 작업을 진행할 수 없습니다. 다시 시도하세요."라는 의미의 특별한 에러 코드를 반환할 것입니다.
따라서 앞서 본 두 개의 소켓을 이용한 예제는 다음과 같이 만들 수 있을 것입니다.

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Bad Example: busy-polling all sockets
[code,C]
------
/* This will work, but the performance will be unforgivably bad. */
int i, n;
char buf[1024];
for (i=0; i < n_sockets; ++i)
    fcntl(fd[i], F_SETFL, O_NONBLOCK);

while (i_still_want_to_read()) {
    for (i=0; i < n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n == 0) {
            handle_close(fd[i]);
        } else if (n < 0) {
            if (errno == EAGAIN)
                 ; /* The kernel didn't have any data for us to read. */
            else
                 handle_error(fd[i], errno);
         } else {
            handle_input(fd[i], buf, n);
         }
    }
}
------

이제 nonblocking 소켓을 사용하게 되었으니 위 코드는 _동작_하긴 하지만, 간신히 돌아갑니다.
성능이 형편 없을 텐데, 두 가지 이유가 있습니다.
첫째, 두 개의 연결 중 하나에 읽을 데이터가 없다면 모든 CPU 사이클을 사용하며 무한 루프에 빠질 것입니다.
둘째, 이 방식으로 한두 개 이상의 연결을 처리하려고 한다면
들어온 데이터가 있던 없던 각 연결에 대한 커널 호출을 하게 됩니다.
그럼 어떻게 해야 커널에게 "이 소켓들이 데이터를 줄 준비가 될 때까지 기다리고, 준비가 되었다면 알려달라"고
할 수 있을까요?

이 문제를 풀기 위해 사람들이 아직도 사용하는 가장 오래된 해결책은 select()입니다.
select()는 3개의 fds 집합(bit 배열)을 인자로 받습니다.
각각은 읽기(reading), 쓰기(writing), 예외(exceptions)를 위한 fd 배열 입니다.
select()는 이 배열 중 하나가 준비될 때까지 기다렸다가 이 배열에 사용 준비가 된 소켓만
남도록 변경해줍니다.

아래는 select()를 이용한 read() 예제입니다.

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
.Example: Using select
[code,C]
------
/* If you only have a couple dozen fds, this version won't be awful */
fd_set readset;
int i, n;
char buf[1024];

while (i_still_want_to_read()) {
    int maxfd = -1;
    FD_ZERO(&readset);

    /* Add all of the interesting fds to readset */
    for (i=0; i < n_sockets; ++i) {
         if (fd[i]>maxfd) maxfd = fd[i];
         FD_SET(i, &readset);
    }

    /* Wait until one or more fds are ready to read */
    select(maxfd+1, &readset, NULL, NULL, NULL);

    /* Process all of the fds that are still set in readset */
    for (i=0; i < n_sockets; ++i) {
        if (FD_ISSET(fd[i], &readset)) {
            n = recv(fd[i], buf, sizeof(buf), 0);
            if (n == 0) {
                handle_close(fd[i]);
            } else if (n < 0) {
                if (errno == EAGAIN)
                     ; /* The kernel didn't have any data for us to read. */
                else
                     handle_error(fd[i], errno);
             } else {
                handle_input(fd[i], buf, n);
             }
        }
    }
}
------


그리고 이번에는 앞서 보았던 ROT13 서버를 select()를 이용하여 다시 구현한 예제입니다.

//BUILD: SKIP
.Example: select()-based ROT13 server
[code,C]
------
include::examples_01/01_rot13_server_select.c[]
------

하지만 아직 끝난게 아닙니다. select()에서 사용하는 bit 배열을 생성하고 읽기 위해
가장 큰 fd에 비례한 시간이 걸리기 때문에, 소켓 개수가 많아 질수록 select() 호출 성능이
끔찍할 정도로 느려지게 됩니다.  footnote:[프로그래머 입장에서는 bit 배열 생성과 읽는데 걸리는 시간이 
select() 호출 시 제공한 fd 개수에 비례합니다. 하지만 커널 입장에서는 bit 배열을 읽는데 걸리는 시간이 
bit 배열에 존재하는 가장 큰 fd에 비례한 시간이 걸리게 되지요.
이는 select()에 넘겨준 fd 배열의 크기와는 상관 없이, _전체 프로그램에서 사용되는 fd의 총 개수_에 가깝기 때문입니다.]

운영체제에 따라 select를 대체할 수 있는 함수를 제공하기도 합니다.
poll(), epoll(), kqueue(), evports, /dev/poll 등이 그것인데,
모두 select() 보다 좋은 성능을 제공합니다. 그리고 poll()을 제외하면
세 개의 fd 배열 중 하나에 소켓을 추가, 소켓 하나를 삭제, 세 개의 fd 배열 중 I/O를 수행할 준비가 된
소켓 하나를 알려 주는 데 모두 O(1)에 해당하는 성능을 제공합니다.

불행히도 성능 좋은 인터페이스 중 어떤 것도 통일된 표준이 아닙니다.
Linux는 epoll(), Darwin을 포함한 BSD Unix 계열은 kqueue(),
Solaris는 evports와 /dev/poll을 제공합니다.
_그리고 각 운영체제는 다른 계열의 운영체제가 제공하는 인테페이스를 제공하지 않습니다_.
이 때문에 이식성 있는 고성능 비동기 어플리케이션을 작성하고자 한다면,
이러한 인터페이스 모두를 wrapping 하면서 각각이 최고의 효율을
낼 수 있도록 하는 추상화가 필요합니다.

그리고 이것이 바로 Libevent의 저수준 API가 제공하는 것입니다.
Libevent의 저수준 API는 다양한 select() 대체 인터페이스 중 
현재 동작하는 컴퓨터에서 사용 가능한 가장 효율적인 버전을 사용하면서
일관된 인터페이스를 제공합니다.

다음은 비동기식 ROT13 서버의 계속된 또다른 버전입니다.
이번에는 select() 대신 Libevent 2를 사용하게 됩니다.
이제 fd 배열이 사라졌음을 확인해주세요. 대신, 우리는 event_base 구조체를 통해
이벤트를 등록/삭제합니다.
이들은 select(), poll(), epoll(), kqueue() 등으로 구현될 수도 있습니다.

//BUILD: SKIP
.Example: A low-level ROT13 server with Libevent
[code,C]
-------
include::examples_01/01_rot13_server_libevent.c[]
-------

(코드에서 확인해 두어야 할 것들 : 소켓 타입을 "int"가 아닌 evutil_fd_t를 사용하였다.
소켓을 nonblocking으로 만들기 위해 fcntl(O_NONBLOCK) 대신 evutil_make_socket_nonblocking을 호출하였다.
이렇게 함으로써 우리 코드가 Unix API와 Win32 네트워킹 API가 달라지는 부분에서도 호환되게 하였다.)

What about convenience?  (and what about Windows?)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

아마도 여러분은 우리 코드가 좀 더 효율적으로 변했고,
동시에 좀 더 복잡해졌음을 알아차리셨을 겁니다.

프로세스를 forking 하던 때로 돌아가면,

우린 각 연결에 대한 버퍼를 관리할 필요가 없었습니다 : 
각 프로세스에는 각각 분리되어 스택이 할당된 버퍼가 있었으니까요.

각 소켓이 읽기 상태인지, 쓰기 상태인지 명시적으로 매번 확인(track)할 필요도 없었습니다 :
코드 흐름에 따라 암묵적으로 그렇게 되고 있었으니까요.

또한 얼마나 많은 작업이 완료 되었는지 확인하기 위한 구조체도 필요 없었습니다 :
그냥 루프와 스택 변수를 사용하면 되었으니까요.

게다가, 여러분이 Windows 상의 네트워킹에 깊은 경험이 있다면
앞서 살펴 보았던 예제들에서 Libevent가 최적의 성능을 보이지 못할지도 모른다는 사실을
알아차리게 될 것입니다.
Windows에서는 고속 비동기 IO를 위한 방식이 select() 같은 인터페이스가 아니라
IOCP(IO Completion Ports) API를 사용하는 것입니다.
다른 모든 고속 네트워킹 API와 달리,
IOCP는 프로그램에서 수행해야 할 작업에 쓰일 소켓이 _준비(ready)_ 되었을 때
프로그램으로 알림(alert)을 주지 않습니다.
대신, 프로그램은 Windows 네트워크 스택에 네트워크 작업을 _시작(start)_하라고 말하고
나중에 그 작업이 완료 되었을 때 IOCP가 프로그램에게 그 사실을 알려 줍니다.

다행히도 Libevent 2의 "bufferevents" 인터페이스로 위의 두가지 이슈를 해결할 수 있습니다.
이를 통해 프로그램을 작성하기 훨씬 간단해지고,
Libevent가 Windows, _그리고_ Unix 환경에서 효율적인 구현을 할 수 있도록 하는 인터페이스를 제공합니다. 

아래는 bufferevents API를 이용한 마지막 ROT13 서버입니다.

//BUILD: SKIP
.Example: A simpler ROT13 server with Libevent
[code,C]
-------
include::examples_01/01_rot13_server_bufferevent.c[]
-------

How efficient is all of this, really?
이 모든게 얼마나 효과적입니까? 안그래요?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

XXXX write an efficiency section here.  The benchmarks on the libevent
page are really out of date.



